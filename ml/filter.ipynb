{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2a7ba24a' '44dd5d31' '92511e53' '1bf79614' 'cf88d785' '168dd1ac'\n",
      " '7cc59678' '16230396' '5c1034cf' '7848bec2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=4, n_times=373032\n",
      "    Range : 0 ... 373031 =      0.000 ...  1457.152 secs\n",
      "Ready.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.1 - 50 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.10\n",
      "- Lower transition bandwidth: 0.10 Hz (-6 dB cutoff frequency: 0.05 Hz)\n",
      "- Upper passband edge: 50.00 Hz\n",
      "- Upper transition bandwidth: 12.50 Hz (-6 dB cutoff frequency: 56.25 Hz)\n",
      "- Filter length: 8449 samples (33.004 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 59 - 61 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 59.35\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 59.10 Hz)\n",
      "- Upper passband edge: 60.65 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 60.90 Hz)\n",
      "- Filter length: 1691 samples (6.605 s)\n",
      "\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Kept 87 puzzles, skipped 3 puzzles\n",
      "Creating RawArray with float64 data, n_channels=4, n_times=1537\n",
      "    Range : 0 ... 1536 =      0.000 ...     6.000 secs\n",
      "Ready.\n",
      "Not setting metadata\n",
      "6 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 6 events and 257 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v7/_b3rbd3s15b4q431xdmn_6hm0000gn/T/ipykernel_10394/1031772095.py:18: RuntimeWarning: Omitted 1534 annotation(s) that were outside data range.\n",
      "  raw.set_annotations(annotations)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.5s finished\n",
      "  0%|          | 0/10 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4, 50, 257)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(1028, 50, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m pdf \u001b[39m=\u001b[39m mne_filt(pdf)\n\u001b[1;32m     78\u001b[0m raw_dfs \u001b[39m=\u001b[39m separated_puzzles(pdf)\n\u001b[0;32m---> 79\u001b[0m filt_dfs \u001b[39m=\u001b[39m [mne_psd_morlet(raw_df) \u001b[39mfor\u001b[39;49;00m raw_df \u001b[39min\u001b[39;49;00m raw_dfs]\n\u001b[1;32m     81\u001b[0m \u001b[39m# total_neg = 0\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m# total = 0\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m# for filt_df in filt_dfs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[39m#all_raw.extend(raw_dfs)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m all_psd\u001b[39m.\u001b[39mextend(filt_dfs)    \n",
      "Cell \u001b[0;32mIn[20], line 79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m pdf \u001b[39m=\u001b[39m mne_filt(pdf)\n\u001b[1;32m     78\u001b[0m raw_dfs \u001b[39m=\u001b[39m separated_puzzles(pdf)\n\u001b[0;32m---> 79\u001b[0m filt_dfs \u001b[39m=\u001b[39m [mne_psd_morlet(raw_df) \u001b[39mfor\u001b[39;00m raw_df \u001b[39min\u001b[39;00m raw_dfs]\n\u001b[1;32m     81\u001b[0m \u001b[39m# total_neg = 0\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m# total = 0\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m# for filt_df in filt_dfs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[39m#all_raw.extend(raw_dfs)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m all_psd\u001b[39m.\u001b[39mextend(filt_dfs)    \n",
      "Cell \u001b[0;32mIn[19], line 64\u001b[0m, in \u001b[0;36mmne_psd_morlet\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     61\u001b[0m freq_labels \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfreq_\u001b[39m\u001b[39m{\u001b[39;00mfreq\u001b[39m}\u001b[39;00m\u001b[39mHz\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m freq \u001b[39min\u001b[39;00m freqs]\n\u001b[1;32m     63\u001b[0m \u001b[39m# Create a DataFrame from the power array\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m df_power \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(power_2d, columns\u001b[39m=\u001b[39;49mfreq_labels)\n\u001b[1;32m     66\u001b[0m \u001b[39m# Create arrays representing the epoch, time, and channel for each row\u001b[39;00m\n\u001b[1;32m     67\u001b[0m epochs_array   \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrepeat(np\u001b[39m.\u001b[39marange(power\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), power\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39mpower\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\u001b[39m*\u001b[39mpower\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:758\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    747\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    748\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    749\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    755\u001b[0m             copy\u001b[39m=\u001b[39m_copy,\n\u001b[1;32m    756\u001b[0m         )\n\u001b[1;32m    757\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    759\u001b[0m             data,\n\u001b[1;32m    760\u001b[0m             index,\n\u001b[1;32m    761\u001b[0m             columns,\n\u001b[1;32m    762\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    763\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    764\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[1;32m    765\u001b[0m         )\n\u001b[1;32m    767\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/construction.py:315\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    309\u001b[0m     _copy \u001b[39m=\u001b[39m (\n\u001b[1;32m    310\u001b[0m         copy_on_sanitize\n\u001b[1;32m    311\u001b[0m         \u001b[39mif\u001b[39;00m (dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m astype_is_view(values\u001b[39m.\u001b[39mdtype, dtype))\n\u001b[1;32m    312\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m     values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(values, copy\u001b[39m=\u001b[39m_copy)\n\u001b[0;32m--> 315\u001b[0m     values \u001b[39m=\u001b[39m _ensure_2d(values)\n\u001b[1;32m    317\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39m# by definition an array here\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     values \u001b[39m=\u001b[39m _prep_ndarraylike(values, copy\u001b[39m=\u001b[39mcopy_on_sanitize)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/construction.py:570\u001b[0m, in \u001b[0;36m_ensure_2d\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    568\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape((values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\n\u001b[1;32m    569\u001b[0m \u001b[39melif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 570\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMust pass 2-d input. shape=\u001b[39m\u001b[39m{\u001b[39;00mvalues\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    571\u001b[0m \u001b[39mreturn\u001b[39;00m values\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(1028, 50, 6)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy.signal import iirnotch, butter, filtfilt, welch\n",
    "from scipy.stats import kurtosis\n",
    "from mne.time_frequency import tfr_array_morlet\n",
    "import mne\n",
    "from library import *\n",
    "from tqdm import tqdm\n",
    "from autoreject import get_rejection_threshold\n",
    "\n",
    "# prev work - 77%ml in r with: \n",
    "# notch - 60\n",
    "# bp - 1-40\n",
    "# FREQUENCY_BANDS = {'delta': (1, 4), 'theta': (4, 8), 'alpha': (7.5, 13), 'beta': (13, 30), 'gamma': (30, 44)}\n",
    "# spike removal\n",
    "# avg ref\n",
    "\n",
    "def separated_puzzles(df):    \n",
    "    df['block_id'] = df['value'].str.contains('puzzle_loaded').cumsum()\n",
    "\n",
    "    skipped = 0\n",
    "    kept = 0    \n",
    "    pzls = []\n",
    "    for blockid, block in df.groupby('block_id'):     \n",
    "        b = block.copy().reset_index(drop=True)\n",
    "\n",
    "        startid = b['value'].str.contains('puzzle_loaded').idxmax()\n",
    "        endid = b['value'].str.contains('puzzle_finished').idxmax()\n",
    "        \n",
    "        # trial loaded but before first puzzle, or no puzzle finished event\n",
    "        if endid == 0 or endid == None: \n",
    "            continue        \n",
    "        \n",
    "        b = b.iloc[:endid + 1, :]\n",
    "\n",
    "        puzzle_data = b.loc[endid, 'value']\n",
    "        puzzle_data = {p.split(':')[0].strip(): p.split(':')[1].strip() for p in puzzle_data.split(';')}\n",
    "\n",
    "        if 'RESULT' not in puzzle_data or puzzle_data['RESULT'] == 'timeout':\n",
    "            skipped += 1\n",
    "            continue  \n",
    "\n",
    "        \n",
    "        # adds all the puzzle info; may be useful later.\n",
    "        # for key in puzzle_data.keys():\n",
    "        #     b[key] = puzzle_data[key]\n",
    "\n",
    "        # QUIRK: elo bin is in the end string, but ACTUAL elo value is only in the start string. \n",
    "        start_data = b.loc[startid, 'value']\n",
    "        start_data = {p.split(':')[0].strip(): p.split(':')[1].strip() for p in start_data.split(';')}\n",
    "        puzzlestr = start_data['PUZZLESTR']\n",
    "        b['elo'] = puzzlestr.split(',')[3]\n",
    "\n",
    "        b['elo_bin'] = puzzle_data['ELO']\n",
    "        b['solved'] = puzzle_data['RESULT'] == 'solved'\n",
    "        b = b.drop(columns=['value'])\n",
    "\n",
    "        pzls.append(b)        \n",
    "        kept += 1\n",
    "\n",
    "    print(f'Kept {kept} puzzles, skipped {skipped} puzzles')\n",
    "    return pzls\n",
    "\n",
    "df = pd.read_csv(DBFNAME, dtype={'pid':str})\n",
    "df['value'] = df['value'].astype(str)\n",
    "\n",
    "print(df['pid'].unique())\n",
    "\n",
    "all_raw = []\n",
    "all_psd = []\n",
    "for p in tqdm(df['pid'].unique()):\n",
    "   # print(p)\n",
    "    pdf = df[df['pid'] == p].copy().reset_index(drop=True)    \n",
    "\n",
    "    pdf = mne_filt(pdf)\n",
    "\n",
    "    raw_dfs = separated_puzzles(pdf)\n",
    "    #filt_dfs = [mne_psd_morlet(raw_df) for raw_df in raw_dfs]\n",
    "\n",
    "    # total_neg = 0\n",
    "    # total = 0\n",
    "    # for filt_df in filt_dfs:\n",
    "    #     # print number of cells that contain a negative value \n",
    "    #     is_negative = filt_df[[key for key in filt_df.keys() if key not in ['timestamp', 'pid', 'elo', 'block_id',  'solved']]] < 0\n",
    "    #     total_neg += is_negative.sum().sum()\n",
    "    #     total += filt_df.shape[0] * filt_df.shape[1]\n",
    "    # print(\"-------------------------------------------------------------\")\n",
    "    # print(f\"For p {p} - {total_neg / total:.3f}% negative: {total_neg} negative cells out of {total} total cells.\")\n",
    "    # print(\"-------------------------------------------------------------\")\n",
    "    \n",
    "    all_raw.extend(raw_dfs)\n",
    "    #all_psd.extend(filt_dfs)    \n",
    "\n",
    "#psd_df = pd.concat(all_psd).reset_index(drop=True)\n",
    "raw_df = pd.concat(all_raw).reset_index(drop=True)\n",
    "\n",
    "#psd_df.to_csv(DBFNAME.split('.csv')[0] + '_psd_morlet' + FILTNAME, index=False)\n",
    "raw_df.to_csv(DBFNAME.split('.csv')[0] + '_raw_' + FILTNAME, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_spikes(data, window_size=5, threshold_multiplier=4):\n",
    "    data_mavg = data.rolling(window=window_size, center=True).mean()\n",
    "    residuals = data - data_mavg\n",
    "    threshold = threshold_multiplier * residuals.std()\n",
    "    spikes = np.abs(residuals) > threshold\n",
    "    return spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mne_filt(df):    \n",
    "    data = df[CH_NAMES].T.values           \n",
    "    \n",
    "    info = mne.create_info(ch_names=CH_NAMES, sfreq=SAMPLING_RATE, ch_types='eeg')\n",
    "    raw = mne.io.RawArray(data, info)\n",
    "\n",
    "    raw.filter(l_freq=.1, h_freq=50, filter_length='auto') # from 1-40.; 77%ML in r. \n",
    "    raw.notch_filter(freqs=60, filter_length='auto')\n",
    "   \n",
    "    # Re-reference the data to the average of all channels\n",
    "    raw.set_eeg_reference(ref_channels='average', projection=False)\n",
    "    \n",
    "    df_transformed = pd.DataFrame(raw.get_data().T, columns=CH_NAMES)          \n",
    "    df_transformed['timestamp'] = df['timestamp'] \n",
    "    df_transformed['pid'] = df['pid']  \n",
    "    df_transformed['value'] = df['value']  \n",
    "    assert(df_transformed.shape == df.shape)\n",
    "\n",
    "    return df_transformed    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mne_psd(df):\n",
    "\n",
    "    data = df[CH_NAMES].values.T\n",
    "    \n",
    "    # Parameters\n",
    "    window_length = 256  # Hamming window size\n",
    "    overlap = 0.9        # Overlap percentage\n",
    "    step_size = int(window_length * (1 - overlap)) #// 10 # 128-hz new output. //10 -> 128hz; now ~12.8hz\n",
    "    nfft = 256           # Number of FFT points\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    power_data = []\n",
    "\n",
    "    # Slide the window over time and calculate the power values for each frequency band\n",
    "    for start_idx in range(0, data.shape[1] - window_length, step_size):\n",
    "        window_data = data[:, start_idx:start_idx + window_length]\n",
    "\n",
    "        # Calculate power spectral density using Welch's method\n",
    "        freqs, psd = welch(window_data, fs=SAMPLING_RATE, window='hamming', \n",
    "                           nperseg=nfft, noverlap=int(nfft * overlap), scaling='density')\n",
    "        \n",
    "        row_data = {}\n",
    "        for band, (low_freq, high_freq) in FREQUENCY_BANDS.items():\n",
    "            band_mask = np.logical_and(freqs >= low_freq, freqs <= high_freq)\n",
    "            band_psd = psd[:, band_mask]\n",
    "            band_power = np.log(np.sum(band_psd, axis=1)) / np.log(10) # -> for output in bels\n",
    "\n",
    "            for ch_idx, ch_name in enumerate(CH_NAMES):\n",
    "                row_data[f'{band}.{ch_name}'] = band_power[ch_idx]\n",
    "\n",
    "        power_data.append(row_data)\n",
    "\n",
    "    psd_df = pd.DataFrame(power_data)\n",
    "\n",
    "    # interpolate timestamps to the psd transformed data\n",
    "    start_time = df['timestamp'].iloc[0]\n",
    "    end_time = df['timestamp'].iloc[-1]\n",
    "    num_datapoints = len(power_data)\n",
    "    psd_df['timestamp'] = np.linspace(start_time, end_time, num_datapoints)\n",
    "    \n",
    "    # these are all the same for each puzzle, so all good.\n",
    "    psd_df['pid'] = df['pid']\n",
    "    psd_df['elo'] = df['elo']\n",
    "    psd_df['elo_bin'] = df['elo_bin']\n",
    "    psd_df['block_id'] = df['block_id']\n",
    "    psd_df['solved'] = df['solved']\n",
    "\n",
    "    return psd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mne_psd_morlet(df):  \n",
    "    \"\"\"\n",
    "    df is one puzzle's worth of data. \n",
    "    https://mne.tools/stable/auto_tutorials/time-freq/20_sensors_time_frequency.html#time-frequency-analysis-power-and-inter-trial-coherence\n",
    "    \"\"\"  \n",
    "    data = df[CH_NAMES].values.T\n",
    "    \n",
    "    timestamps = df['timestamp'].values\n",
    "    timestamps -= timestamps[0]\n",
    "    time_diffs = np.diff(timestamps) / 1e3 # in seconds\n",
    "\n",
    "    annotations = mne.Annotations(onset=timestamps[:-1],\n",
    "                                  duration=time_diffs,\n",
    "                                  description=['EDGE'] * len(time_diffs))\n",
    "\n",
    "    info = mne.create_info(ch_names=CH_NAMES, sfreq=SAMPLING_RATE, ch_types='eeg')\n",
    "    raw = mne.io.RawArray(data, info)\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    events = mne.make_fixed_length_events(raw, duration=1)\n",
    "    epochs = mne.Epochs(raw, events, tmin=0, tmax=0.999, baseline=None)\n",
    "\n",
    "    freqs = np.arange(1, 51)  # Frequencies from 1 to 50 Hz\n",
    "    n_cycles = freqs / 2. \n",
    "\n",
    "    tfr = mne.time_frequency.tfr_morlet(epochs, freqs=freqs, n_cycles=n_cycles, \n",
    "                                        average=False, return_itc=False, n_jobs=-1, \n",
    "                                        use_fft=True)\n",
    "\n",
    "    # convert to psd\n",
    "    power = np.abs(tfr.data)**2\n",
    "\n",
    "    # convert to log scale (bels)\n",
    "    log_power = 10 * np.log10(power)\n",
    "    print(log_power.shape)\n",
    "\n",
    "    # band_power_data = []\n",
    "    # for band, (low_freq, high_freq) in FREQUENCY_BANDS.items():\n",
    "    #     band_mask = np.logical_and(freqs >= low_freq, freqs <= high_freq)\n",
    "    #     band_psd = psd[:, :, band_mask]\n",
    "    #     band_power = np.log10(np.sum(band_psd, axis=2))\n",
    "    #     band_power_data.append(band_power)\n",
    "\n",
    "    # result_data = []\n",
    "\n",
    "    # for epoch_idx in range(band_power_data[0].shape[0]):\n",
    "    #     epoch_data = {}\n",
    "    #     for band_idx, (band, (low_freq, high_freq)) in enumerate(FREQUENCY_BANDS.items()):\n",
    "    #         for ch_idx, ch_name in enumerate(CH_NAMES):\n",
    "    #             epoch_data[f'{band}.{ch_name}'] = band_power_data[band_idx][epoch_idx, ch_idx]\n",
    "    #     result_data.append(epoch_data)\n",
    "\n",
    "#    psd_df = pd.DataFrame(result_data)   \n",
    " \n",
    "    log_power = log_power.transpose(0, 2, 1, 3)  # Now the shape is, for example (6, 50, 4, 257)\n",
    "\n",
    "    # Reshape the power array to 2D (frequencies x everything else)\n",
    "    power_2d = log_power.reshape((log_power.shape[0], log_power.shape[1], -1)).T\n",
    "\n",
    "    # Create frequency labels for the columns\n",
    "    freq_labels = [f'freq_{freq}Hz' for freq in freqs]\n",
    "\n",
    "    # Create a DataFrame from the power array\n",
    "    df_power = pd.DataFrame(power_2d, columns=freq_labels)\n",
    "\n",
    "    # Create arrays representing the epoch, time, and channel for each row\n",
    "    epochs_array   = np.repeat(np.arange(power.shape[0]), power.shape[1]*power.shape[2]*power.shape[3])\n",
    "    ch_names_array = np.tile(np.repeat(CH_NAMES, power.shape[2]*power.shape[3]), power.shape[0]*power.shape[1])\n",
    "    times_array    = np.tile(np.repeat(tfr.times, len(CH_NAMES)), power.shape[0]*power.shape[1])\n",
    "\n",
    "    # Add these as columns to the DataFrame\n",
    "    df_power['epoch']   = epochs_array\n",
    "    df_power['time']    = times_array\n",
    "    df_power['channel'] = ch_names_array\n",
    "\n",
    "    # Reorder the columns so the epoch, time, and channel are first\n",
    "    df_power = df_power[['epoch', 'time', 'channel'] + freq_labels]\n",
    "\n",
    "\n",
    "    # # interpolate timestamps to the psd transformed data    \n",
    "    # start_time = df['timestamp'].iloc[0]\n",
    "    # end_time = df['timestamp'].iloc[-1]\n",
    "    # num_datapoints = psd_df.shape[0]\n",
    "    # psd_df['timestamp'] = np.linspace(start_time, end_time, num_datapoints)\n",
    "    \n",
    "    # these are all the same for each puzzle, so all good.\n",
    "    psd_df['pid']      = df['pid']\n",
    "    psd_df['elo']      = df['elo']\n",
    "    psd_df['elo_bin']  = df['elo_bin']\n",
    "    psd_df['block_id'] = df['block_id']\n",
    "    psd_df['solved']   = df['solved']\n",
    "    \n",
    "    return psd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neuraldatascience.io/7-eeg/erp_artifacts.html\n",
    "def mne_ica(df):\n",
    "    data = df[CH_NAMES].values.T\n",
    "    info = mne.create_info(ch_names=CH_NAMES, sfreq=SAMPLING_RATE, ch_types='eeg')\n",
    "    raw = mne.io.RawArray(data, info)\n",
    "\n",
    "    print(mne.channels.make_standard_montage('standard_1020', head_size='auto'))\n",
    "\n",
    "    ch_coords = np.array([\n",
    "                            [-0.2852, 0.8777, -0.3826], #[-0.5, -0.5, 0],  # TP9\n",
    "                            [0.8090, 0.5878, 0.0000], #[-0.5,  0.5, 0],  # AF7\n",
    "                            [0.8090, -0.5878, 0.0000],#[ 0.5,  0.5, 0],  # AF8\n",
    "                            [-0.2853, -0.8777, -0.3826]#[ 0.5, -0.5, 0]   # TP10\n",
    "                        ])\n",
    "\n",
    "    # Create a DigMontage object\n",
    "    dig_montage = mne.channels.make_dig_montage(\n",
    "        ch_pos=dict(zip(CH_NAMES, ch_coords)),\n",
    "        coord_frame='head'\n",
    "    )\n",
    "   \n",
    "    raw.set_montage(dig_montage)\n",
    "    \n",
    "    ica_low_cut = 1.0       # For ICA, we filter out more low-frequency power\n",
    "    hi_cut  = 30\n",
    "\n",
    "    raw_ica = raw.copy().filter(ica_low_cut, hi_cut)\n",
    "\n",
    "    tstep = 1.0\n",
    "    events_ica = mne.make_fixed_length_events(raw_ica, duration=tstep)\n",
    "    epochs_ica = mne.Epochs(raw_ica, events_ica,\n",
    "                            tmin=0.0, tmax=tstep,\n",
    "                            baseline=None,\n",
    "                            preload=True)\n",
    "\n",
    "    reject = get_rejection_threshold(epochs_ica)\n",
    "    reject\n",
    "\n",
    "    ica_n_components = .99   # Specify n_components as a decimal to set % explained variance\n",
    "\n",
    "    ica = mne.preprocessing.ICA(n_components=ica_n_components, random_state=RANDOM_STATE)\n",
    "    ica.fit(epochs_ica, reject=reject, tstep=tstep )\n",
    "\n",
    "    ica.plot_properties(epochs_ica, picks=range(0, ica.n_components_), psd_args={'fmax': hi_cut});\n",
    "\n",
    "\n",
    "    ica_z_thresh = 1.96 \n",
    "    eog_indices, eog_scores = ica.find_bads_eog(raw_ica, \n",
    "                                                ch_name=['probe-1', 'probe-2'], \n",
    "                                                threshold=ica_z_thresh)\n",
    "    ica.exclude = eog_indices\n",
    "\n",
    "    ica.plot_scores(eog_scores)\n",
    "    \n",
    "\n",
    "    # # Get the ICA component sources\n",
    "    # sources = ica.get_sources(raw)\n",
    "\n",
    "    # # Compute the kurtosis scores for each component\n",
    "    # kurt_scores = kurtosis(sources.get_data(), axis=1)    \n",
    "    # eog_indices = np.where(kurt_scores > 7)[0]\n",
    "        \n",
    "    # raw_clean = ica.apply(raw.copy(), exclude=eog_indices)\n",
    "\n",
    "    # data_clean = raw_clean.get_data()\n",
    "    # data_clean.plot(title=\"cleaned raw data after ICA\", scalings=\"auto\") #,start=0, duration=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2a7ba24a' '44dd5d31' '92511e53' '1bf79614' 'cf88d785' '168dd1ac'\n",
      " '7cc59678' '16230396' '5c1034cf' '7848bec2']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DBFNAME, dtype={'pid':str})\n",
    "print(df['pid'].unique())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
